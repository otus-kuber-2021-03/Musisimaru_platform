# Часть 0-1 | Подготовка инструментов и среды

**ОС:** windows 10

- **ставим** `kubectl` . Менеджерами пакетов и прочими утилитами ставиться без ошибок не захотел. 
**скачал** exe-шник, добавил переменную среды руками.

- **ставим** `minikube` . Пакеты и командлеты тоже сбоят. Скачал установщик. Сработало.
**запускаем** сначала используя `hyperV` . Вылетает в ошибку. Ошибка трудно диагностируемая. Удаляем и зачищаем экземпляр
**запускаем** используя `docker`.

    ![minikube-01](https://user-images.githubusercontent.com/14264908/114279369-f5f32500-9a3c-11eb-982b-9d7263fe8657.png)

    Контейнер миникуба успешно запущен. Но есть проблема - если проверим статус компонент. То видим картину

    ![minikube-02](https://user-images.githubusercontent.com/14264908/114279373-fd1a3300-9a3c-11eb-8e72-189ba7f00889.png)

    **решаем проблему** - подключаемся к контейнеру с миникубом и переходим в директорию `/etc/kubernetes/manifests/` . Там удаляем из файлов `kube-scheduler.yaml` и `kube-controller-manager.yaml` строки содержащие  `--port=0` . После чего перезапустить кластер. Для этого, например, выполняем

    ```bash
    sed -i '/- --port=0/d' ./kube-scheduler.yaml
    sed -i '/- --port=0/d' ./kube-controller-manager.yaml
    sudo systemctl restart kubelet.service
    ```

    **проблема ушла**

    ![minikube-03](https://user-images.githubusercontent.com/14264908/114279380-04d9d780-9a3d-11eb-9ca6-5167e5660cf8.png)


- **ставим** `dashboard` . Применился штатно.
**добавляем** аккаунт для входа, наделяем правами *(пока не разбираемся, просто списываем из документации требуемые)*

    ```bash
    kubectl create serviceaccount dashboard-admin-sa
    kubectl create clusterrolebinding dashboard-admin-sa --clusterrole=cluster-admin --serviceaccount=default:dashboard-admin-sa
    ```

    смотрим имя защищенных метаданных созданного экземпляра пользователя и по нему тащим токен для аутентификации и авторизации в дашборде

    ```bash
    kubectl get secrets
    kubectl describe secret dashboard-admin-sa-token-[tail]
    ```

- **ставим** `k9s`. Ни одним штатным средством не удалось заставить его запуститься. Попрбуем потом на виртуалке с `linux` или `ubuntu`

# Часть 0-2 |  Провекрка на отказоустойчивость

Проделал все описанное в задании. Все контейнеры поднялись. Поды поднялись не все. Пропал `storage-provisioner`.

**смотрим** перед удалением

![problem-01](https://user-images.githubusercontent.com/14264908/114279430-56826200-9a3d-11eb-8afa-f234951fc511.png)

**удаляем**

![problem-02](https://user-images.githubusercontent.com/14264908/114279437-5c784300-9a3d-11eb-99f4-04128b4ce192.png)

**осталось** 6. Возможно это норма. Оставим пока что так.

![problem-03](https://user-images.githubusercontent.com/14264908/114279442-60a46080-9a3d-11eb-8edc-6aef7cf4980a.png)

## Причины как они поднимаются

**смотрим** через команду `describe` каждый под. Сравниваем

**пробуем** еще несколько раз их удалять.

**проверяем** какие еще экземпляры есть в пространстве имен `kube-system` и вообще кого мы можем запросить командой `get` .

**курим** по диагонали документацию про каждую из этих сущностей. А также про `priority class name` .

### Итог

**coredns** контроллируется `deployment` контроллером и он связан с сущностью Replicaset. Это видно из его описания, а также если запросить список деплойментов или сетов. У него явно указано сколько должно быть кол-во реплик.

![problem-04](https://user-images.githubusercontent.com/14264908/114279451-69953200-9a3d-11eb-9794-3114fac1e0fa.png)

**kube-proxy** связан с сущностью `Daemonset`. Возможно механизм аналогичен. По крайней мере у него есть схожие атрибуты.

![problem-05](https://user-images.githubusercontent.com/14264908/114279455-6dc14f80-9a3d-11eb-8f1f-f1578a50cbe3.png)

Для остальных подов какой-то **явный** механизм или связь, которые бы следили за состоянием этих подов или кол-вом реплик я не нашел. 

Сравнив между собою описания только выяснил, что у все у них  `priority class name` значением `system-cluster-critical` ([https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/](https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/)). В документации косвенно этим классом характерезуют поды, которые должны подниматься, чтобы кластер был жизнеспособен.

Ну и в отличие от первых двух у остальных в артибуте `Controlled By` значение `Node/minikube`. Так что по аналогии могу предположить, что есть другой контроллер или тот же `deployment` , который следит за тем сколько их должно быть

# Часть 1 | Запускаем свой образ в под кубера

делаем на основе образа веб-сервера апач свой образ и закидываем в **docker hub**

**завел** первую версию манифеста и запускаем под. 

При первом инстансе миникуба у меня был не рабочий кластер, под висел в состоняии подготовки.

При втором оказалось, что я накосячил с созданием пользователя в образе. точнее сказать я его не создал, но переключился на него. поэтому под выпал в статус сначала перезапуска, а потом в что мол `ну, я пытался` . 

**убрал** из образа переключение пользователя. Все зарабоатло.

**добавил** init-контейнер по заданию.

**проверил** результат. Тут у меня постоянно вылетали ошибки, что по запрашиваемому порту нечего пробрасывать. попробовал все возможные комбинации пар из портов `8000`, `8080` и `80` . А также стирал и добавлял IP адрес.

перечитал описание пода. Увидел поле порта, которое было без значение.

добавил в манифест порт. Но перебор снова не помог. Обратился к преподавателю. Получил цу, что у меня не верный порт. 

вернулся к заданию спустя день, сработали

```bash
kubectl port-forward pod/homework-01-web 8080:80
```
| port-forward | kube-forwarder |
| --- | --- |
| ![chapter01-01](https://user-images.githubusercontent.com/14264908/114279524-cf81b980-9a3d-11eb-97d3-3870a76b7f4a.png) | ![chapter01-02](https://user-images.githubusercontent.com/14264908/114279526-d01a5000-9a3d-11eb-861d-b60b3d05961b.png) |


результат:

![chapter01-03](https://user-images.githubusercontent.com/14264908/114279512-c2fd6100-9a3d-11eb-880c-036a9fb5320e.png)

# Часть 2 | Запускаем чужой образ в под кубера

**собрал** образ и залили его в Docker Hub.

**запустил** `pod`

```bash
kubectl run frontend --image musisimaru/otus-hipstershop-01:0.0.1 --restart=Never
```

**создал** файл с манифестом

```bash
kubectl run frontend --image musisimaru/otus-hipstershop-01:0.0.1 --restart=Never --dry-run -o yaml > frontend-pod.yaml
```

**смотрю** статус. Под действительно в статусе `Error`

**смотрю** как он себя ведет в докере. в докере он тоже не запускается. Пишет следующее

```bash
{"message":"Tracing enabled.","severity":"info","timestamp":"2021-04-10T15:47:53.9290147Z"}
{"message":"Profiling enabled.","severity":"info","timestamp":"2021-04-10T15:47:53.9292111Z"}
{"message":"jaeger initialization disabled.","severity":"info","timestamp":"2021-04-10T15:47:53.9293534Z"}
panic: environment variable "PRODUCT_CATALOG_SERVICE_ADDR" not set

goroutine 1 [running]:
main.mustMapEnv(0xc000448000, 0xb1f32c, 0x1c)
	/src/main.go:259 +0x10b
main.main()
	/src/main.go:117 +0x510
```

**выполнил**  то, что подсказывают нам в дз

```bash
kubectl logs frontend
```

Там тот же самый текст. Класс. Берем на вооружение.

**делаем** предположение
Не хватает некой системной переменной среды `PRODUCT_CATALOG_SERVICE_ADDR`

**в проекте** есть папка `kubernetes-manifests` а в ней `frontend.yaml` . Лезем туда и находим

```yaml
- name: PRODUCT_CATALOG_SERVICE_ADDR
  value: "productcatalogservice:3550"
```

**теперь понятно** - ему нужен адерс службы с которой он будет работать в свзяке. Надо вписать хоть что-нибудь. Работать правильно он конечно не будет, т.к. она у нас тупо не запущена, но хотя бы запустится.

**добавляем** в сгенерировнный манифест  именно эту переменную окружения. Запускаем под из него. Снова `Error` . Теперь `panic: environment variable "CURRENCY_SERVICE_ADDR" not set`

**понятно**, ему нужны адреса всех микрофсервисов чтобы проинициализироваться. Чтож, копируем. Перезапускаем

**статус** `Running`

Ура!
